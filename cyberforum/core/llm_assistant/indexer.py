import os
from pathlib import Path

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from .loader import load_documents_from_folder
from .utils import clean_text, count_tokens

INDEX_DIR = "faiss_index"

CURRENT_DIR = Path(__file__).parent
DOCS_DIR = CURRENT_DIR / "docs"


def create_or_load_vectorstore():
    """
    –°–æ–∑–¥–∞—ë—Ç –∏–ª–∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É.
    –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ Django.
    """
    embedding_model = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    )

    if os.path.exists(INDEX_DIR):
        print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∏–Ω–¥–µ–∫—Å–∞...")
        vectorstore = FAISS.load_local(
            INDEX_DIR, embedding_model, allow_dangerous_deserialization=True
        )
        print("‚úÖ –ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω.")
    else:
        print("üÜï –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞...")
        documents = load_documents_from_folder(DOCS_DIR)
        if not documents:
            raise RuntimeError("–ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏!")

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=45,
            chunk_overlap=35,
            length_function=count_tokens,
            separators=["\n\n", "\n", ". ", "! ", "? ", " ", ""],
        )

        chunks = splitter.split_documents(documents)
        for chunk in chunks:
            chunk.page_content = clean_text(chunk.page_content)

        vectorstore = FAISS.from_documents(chunks, embedding_model)
        vectorstore.save_local(INDEX_DIR)
        print(f"‚úÖ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {INDEX_DIR}/")

    return vectorstore
